#!/usr/bin/env python3
import pdfkit
import sys
import os
import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfMerger

NO_SECTION = "DEFAULT_SECTION"


def get_page_title(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        return soup.title.string.strip() if soup.title else "Untitled"
    except Exception as e:
        print(f"An error occurred while fetching the title: {e}")
        return "Untitled"


def download_page_as_pdf(url, full_output_path):
    try:
        pdfkit.from_url(url, full_output_path)
    except Exception as e:
        print(f"An error occurred: {e}")


def process_url(url, counter, section=None):
    title = get_page_title(url)
    print(f"!!!! Found page: {title}")
    # Shorten the title to 20 characters and replace spaces with underscores
    short_title = title[:20].replace(" ", "_")
    output_path = f"{counter}_{short_title}.pdf"
    if section:
        output_dir = os.path.join("scraper_dump", section)
    else:
        output_dir = "scraper_dump"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    full_output_path = os.path.join(output_dir, output_path)
    download_page_as_pdf(url, full_output_path)
    print(f"++++ Saved page as {full_output_path}\n")
    return full_output_path


def merge_files(file_paths, section=None):
    if section:
        print(f"==== Merging the following files for section {section}:")
    else:
        print("==== Merging the following files:")
    for path in file_paths:
        print(path)

    merger = PdfMerger()
    for pdf in file_paths:
        merger.append(pdf)

    if section:
        merged_output_path = os.path.join("scraper_dump", f"{section}.pdf")
    else:
        merged_output_path = os.path.join("scraper_dump", "Merged.pdf")
    merger.write(merged_output_path)
    merger.close()
    print(f"==== Merged PDF saved as {merged_output_path}\n")


def parse_input_file(file_path):
    sections = {}
    current_section = None
    with open(file_path, "r") as file:
        for line in file:
            line = line.strip()
            if line.startswith("#"):
                current_section = line[1:].strip().replace(" ", "_")
                sections[current_section] = []
            elif line:
                if current_section is not None:
                    sections[current_section].append(line)
                else:
                    if NO_SECTION not in sections:
                        sections[NO_SECTION] = []
                    sections[NO_SECTION].append(line)
    return sections


def print_help():
    print("Usage: scraper.py [options] <file_with_urls | single_url>")
    print("Options:")
    print("  -h, --help    Show this help message and exit")
    print(
        "\nIf a file with URLs is provided, the script will process each URL in the file."
    )
    print("If a single URL is provided, the script will process that URL.")
    print(
        "If no file or URL is provided, the script will prompt for URLs interactively."
    )
    print("Enter URLs one by one. Type 'quit' to finish and prompt for merging.")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        input_arg = sys.argv[1]
        if input_arg in ("-h", "--help"):
            print_help()
        elif os.path.isfile(input_arg):
            sections = parse_input_file(input_arg)
            if NO_SECTION in sections and len(sections) == 1:
                # Process as a plain file with just URLs
                output_paths = []
                counter = 1
                for url in sections[NO_SECTION]:
                    output_path = process_url(url, counter)
                    output_paths.append(output_path)
                    counter += 1
                merge_files(output_paths)
            else:
                # Process as a file with sections
                for section, urls in sections.items():
                    output_paths = []
                    counter = 1
                    for url in urls:
                        output_path = process_url(url, counter, section)
                        output_paths.append(output_path)
                        counter += 1
                    merge_files(output_paths, section)
        else:
            output_path = process_url(input_arg, 1)
    else:
        escape_character = "quit"
        counter = 1
        output_paths = []
        print("Enter URLs one by one. Type 'quit' to finish and prompt for merging.")
        while True:
            url = input("Enter URL (or type 'quit' to quit): ")
            if url.lower() == escape_character:
                print("Exiting the program.")
                merge_prompt = input("Merge? (Y/N): ").strip().lower()
                if merge_prompt == "y":
                    merge_files(output_paths)
                break
            if not url:
                print("Error: URL cannot be empty. Please enter a valid URL.")
                continue
            output_path = process_url(url, counter)
            output_paths.append(output_path)
            counter += 1
