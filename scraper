#!/usr/bin/env python3
import pdfkit
import sys
import os
import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfMerger
from concurrent.futures import ThreadPoolExecutor, as_completed

NO_SECTION = "DEFAULT_SECTION"


def get_page_title(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        return soup.title.string.strip() if soup.title else "Untitled"
    except Exception as e:
        print(f"An error occurred while fetching the title: {e}")
        return "Untitled"


def download_page_as_pdf(url, full_output_path):
    try:
        pdfkit.from_url(url, full_output_path)
        print(f"++++ Downloaded PDF: {full_output_path}")
    except Exception as e:
        print(f"An error occurred: {e}")


def process_urls(urls, section=None):
    tasks = []
    counter = 1
    for url in urls:
        title = get_page_title(url)
        print(f"!!!! Found page: {title}")
        # Shorten the title to 20 characters and replace spaces with underscores
        short_title = title[:20].replace(" ", "_")
        output_path = f"{counter}_{short_title}.pdf"
        if section:
            output_dir = os.path.join("scraper_dump", section)
        else:
            output_dir = "scraper_dump"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        full_output_path = os.path.join(output_dir, output_path)
        tasks.append((url, full_output_path))
        counter += 1
    return tasks


def merge_files(file_paths, section=None):
    print()
    if section:
        print(f"==== Merging the following files for section {section}:")
    else:
        print("==== Merging the following files:")
    for path in file_paths:
        print(path)

    merger = PdfMerger()
    for pdf in file_paths:
        merger.append(pdf)

    if section:
        merged_output_path = os.path.join("scraper_dump", f"{section}.pdf")
    else:
        merged_output_path = os.path.join("scraper_dump", "Merged.pdf")
    merger.write(merged_output_path)
    merger.close()
    print(f"==== Merged PDF saved as {merged_output_path}\n")


def parse_input_file(file_path):
    sections = {}
    current_section = None
    with open(file_path, "r") as file:
        for line in file:
            line = line.strip()
            if line.startswith("#"):
                current_section = line[1:].strip().replace(" ", "_")
                sections[current_section] = []
            elif line:
                if current_section is not None:
                    sections[current_section].append(line)
                else:
                    if NO_SECTION not in sections:
                        sections[NO_SECTION] = []
                    sections[NO_SECTION].append(line)
    return sections


def print_help():
    print("Usage: scraper.py [options] <file_with_urls | single_url>")
    print("Options:")
    print("  -h, --help    Show this help message and exit")
    print(
        "\nIf a file with URLs is provided, the script will process each URL in the file."
    )
    print("If a single URL is provided, the script will process that URL.")
    print(
        "If no file or URL is provided, the script will prompt for URLs interactively."
    )
    print("Enter URLs one by one. Type 'quit' to finish and prompt for merging.")


def download_concurrently(tasks):
    output_paths = []
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(download_page_as_pdf, url, path): path
            for url, path in tasks
        }
        for future in as_completed(futures):
            future.result()  # Ensure the thread completes
            output_paths.append(futures[future])
    return output_paths


if __name__ == "__main__":
    if len(sys.argv) > 1:
        input_arg = sys.argv[1]
        if input_arg in ("-h", "--help"):
            print_help()
        elif os.path.isfile(input_arg):
            sections = parse_input_file(input_arg)
            if NO_SECTION in sections and len(sections) == 1:
                # Process as a plain file with just URLs
                tasks = process_urls(sections[NO_SECTION])
                output_paths = download_concurrently(tasks)
                merge_files(output_paths)
            else:
                # Process as a file with sections
                for section, urls in sections.items():
                    tasks = process_urls(urls, section)
                    output_paths = download_concurrently(tasks)
                    merge_files(output_paths, section)
        else:
            tasks = process_urls([input_arg])
            output_paths = download_concurrently(tasks)
    else:
        escape_character = "quit"
        counter = 1
        urls = []
        print("Enter URLs one by one. Type 'quit' to finish and prompt for merging.")
        while True:
            url = input("Enter URL (or type 'quit' to quit): ")
            if url.lower() == escape_character:
                print("Exiting the program.")
                break
            if not url:
                print("Error: URL cannot be empty. Please enter a valid URL.")
                continue
            urls.append(url)
        tasks = process_urls(urls)
        output_paths = download_concurrently(tasks)
        merge_prompt = input("Merge? (Y/N): ").strip().lower()
        if merge_prompt == "y":
            merge_files(output_paths)
